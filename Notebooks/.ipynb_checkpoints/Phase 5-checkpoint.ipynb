{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a1b2c3d4-e5f6-7g8h-9i0j-1k2l3m4n5o6p",
   "metadata": {},
   "source": [
    "# Phase 5: Out-of-Distribution Testing on Turbulent Couette Flow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2c3d4e5-f6g7-8h9i-0j1k-2l3m4n5o6p7q",
   "metadata": {},
   "source": [
    "## This notebook evaluates the generalization capability of our trained ML models on turbulent Couette flow - a flow with fundamentally different driving physics than the channel flow used for training. This out-of-distribution test assesses whether the models have learned true turbulence physics or merely memorized training data patterns."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3d4e5f6-g7h8-9i0j-1k2l-3m4n5o6p7q8r",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4e5f6g7-h8i9-0j1k-2l3m-4n5o6p7q8r9s",
   "metadata": {},
   "source": [
    "# 1. Introduction: The Critical Importance of Out-of-Distribution Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5f6g7h8-i9j0-1k2l-3m4n-5o6p7q8r9s0t",
   "metadata": {},
   "source": [
    "## 1.1 Why Out-of-Distribution Testing Matters\n",
    "\n",
    "### In machine learning for turbulence modeling, a model that performs well on test data from the same distribution as training data does not guarantee true understanding of turbulence physics. The model might simply be interpolating patterns rather than learning fundamental physical relationships.\n",
    "\n",
    "### **Out-of-distribution (OOD) testing** addresses this concern by evaluating the model on flows with:\n",
    "\n",
    "#### - **Different driving mechanisms**: Channel flow is pressure-driven, Couette flow is shear-driven\n",
    "#### - **Different boundary conditions**: Channel flow has stationary walls, Couette flow has moving walls\n",
    "#### - **Different physics**: Despite both being wall-bounded turbulent flows, the energy input mechanisms differ fundamentally\n",
    "\n",
    "### If ML models trained exclusively on channel flow can accurately predict Couette flow, it demonstrates they have learned generalizable turbulence physics rather than flow-specific patterns."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6g7h8i9-j0k1-2l3m-4n5o-6p7q8r9s0t1u",
   "metadata": {},
   "source": [
    "## 1.2 Turbulent Channel Flow vs. Turbulent Couette Flow\n",
    "\n",
    "### **Channel Flow (Training Data)**\n",
    "\n",
    "#### - **Driving Force**: Pressure gradient (∂P/∂x < 0)\n",
    "#### - **Boundary Conditions**: Two parallel stationary walls\n",
    "#### - **Mean Velocity Profile**: Parabolic in laminar regime, logarithmic in turbulent regime\n",
    "#### - **Energy Input**: External pressure gradient drives the flow\n",
    "#### - **Physical Analogy**: Flow in pipes, ducts, or between fixed plates\n",
    "\n",
    "### **Couette Flow (Test Data)**\n",
    "\n",
    "#### - **Driving Force**: Wall motion (top wall moves, bottom wall stationary)\n",
    "#### - **Boundary Conditions**: One moving wall at velocity U_wall, one stationary wall\n",
    "#### - **Mean Velocity Profile**: Nearly linear across channel height in turbulent regime\n",
    "#### - **Energy Input**: Direct shear from moving wall\n",
    "#### - **Physical Analogy**: Flow between a rotating cylinder and stationary outer wall, or conveyor belt systems\n",
    "\n",
    "### Despite these fundamental differences, both flows exhibit turbulent characteristics at high Reynolds numbers, including:\n",
    "\n",
    "#### - Near-wall streaks and vortical structures\n",
    "#### - Logarithmic velocity profile near walls\n",
    "#### - Energy cascade from large to small scales\n",
    "#### - Reynolds stress anisotropy and turbulent production\n",
    "\n",
    "### The question: **Can models trained on channel flow recognize and predict these common turbulence physics in Couette flow?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "g7h8i9j0-k1l2-3m4n-5o6p-7q8r9s0t1u2v",
   "metadata": {},
   "source": [
    "## 1.3 Dataset Information\n",
    "\n",
    "### The Couette flow DNS data is from Lee & Moser (2018), \"Extreme-scale motions in turbulent plane Couette flows\", Journal of Fluid Mechanics.\n",
    "\n",
    "#### - **Reynolds number**: Re_λ ≈ 500 (comparable to Case 4 channel flow test set)\n",
    "#### - **Domain size**: L_x × L_y × L_z = 100π × 2 × 4π (large domain to capture extreme-scale motions)\n",
    "#### - **Resolution**: High-fidelity DNS resolving all turbulent scales from Kolmogorov to integral scales\n",
    "#### - **Flow configuration**: Plane Couette flow between parallel plates\n",
    "\n",
    "### The DNS provides turbulence statistics including:\n",
    "\n",
    "#### - Wall-normal coordinate (y⁺)\n",
    "#### - Reynolds stress anisotropy (b₁₂)\n",
    "#### - Mean velocity gradient (dU/dy)\n",
    "#### - Turbulence kinetic energy dissipation rate (ε)\n",
    "#### - Turbulence kinetic energy (k)\n",
    "#### - Pressure-strain correlation (φ₁₂) - our prediction target"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "h8i9j0k1-l2m3-4n5o-6p7q-8r9s0t1u2v3w",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "i9j0k1l2-m3n4-5o6p-7q8r-9s0t1u2v3w4x",
   "metadata": {},
   "source": [
    "# 2. Library Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "j0k1l2m3-n4o5-6p7q-8r9s-0t1u2v3w4x5y",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import joblib\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.metrics import r2_score, mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "307276ad-3d4a-4761-837b-3c79107ebb52",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore', message='X does not have valid feature')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "k1l2m3n4-o5p6-7q8r-9s0t-1u2v3w4x5y6z",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "l2m3n4o5-p6q7-8r9s-0t1u-2v3w4x5y6z7a",
   "metadata": {},
   "source": [
    "# 3. Data Processing: From .dat to DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "m3n4o5p6-q7r8-9s0t-1u2v-3w4x5y6z7a8b",
   "metadata": {},
   "source": [
    "## 3.1 Understanding the .dat File Format\n",
    "\n",
    "### DNS data files typically contain space or tab-separated numerical columns. The structure may vary depending on the simulation code and post-processing.\n",
    "\n",
    "### **Common .dat file formats:**\n",
    "\n",
    "#### - **With header**: First row contains column names\n",
    "#### - **Without header**: Data starts immediately, column structure must be inferred\n",
    "#### - **Comment lines**: Lines starting with # or % containing metadata\n",
    "\n",
    "### **Expected columns for Couette flow data:**\n",
    "\n",
    "#### The order and availability of columns depends on the specific DNS output. Typical structure includes:\n",
    "\n",
    "#### - y⁺ or y/h: Wall-normal coordinate\n",
    "#### - U⁺: Mean streamwise velocity (normalized)\n",
    "#### - Reynolds stresses: uu, vv, ww, uv (or their normalized forms)\n",
    "#### - Mean velocity gradients: dU/dy\n",
    "#### - Derived quantities: k, ε, b_ij, φ_ij\n",
    "\n",
    "### **Our approach:**\n",
    "#### 1. Load the .dat file and examine its structure\n",
    "#### 2. Identify or compute required features: b₁₂, dU/dy, ε, k\n",
    "#### 3. Extract target variable: φ₁₂\n",
    "#### 4. Save as structured CSV for consistency"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "n4o5p6q7-r8s9-0t1u-2v3w-4x5y6z7a8b9c",
   "metadata": {},
   "source": [
    "## 3.2 Load and Inspect Couette Flow Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "o5p6q7r8-s9t0-1u2v-3w4x-5y6z7a8b9c0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = \"../DNS (Lee and Moser 2015)/Couette Flow (Re -500)/LM_Couette_R0500_100PI_mean_prof.dat\"    # file path is huge\n",
    "mean_prof_500_df = pd.read_csv(\n",
    "    file,\n",
    "    sep='\\\\s+',\n",
    "    comment='%',\n",
    "    header=None\n",
    ")\n",
    "mean_prof_500_df.columns = [\n",
    "    'y/delta',\n",
    "    'y^+',\n",
    "    'U',\n",
    "    'dU/dy',\n",
    "    'W',\n",
    "    'P'\n",
    "]\n",
    "\n",
    "mean_prof_500_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd7fc0c6-fc13-4b67-84ec-52c799afa106",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = \"../DNS (Lee and Moser 2015)/Couette Flow (Re -500)/LM_Couette_R0500_100PI_vel_fluc_prof.dat\"    # file path is huge\n",
    "vel_fluc_500_df = pd.read_csv(\n",
    "    file,\n",
    "    sep='\\\\s+',\n",
    "    comment='%',\n",
    "    header=None\n",
    ")\n",
    "vel_fluc_500_df.columns = [\n",
    "    'y/delta',\n",
    "    'y^+',\n",
    "    \"u'u'\",\n",
    "    \"v'v'\",\n",
    "    \"w'w'\",\n",
    "    \"u'v'\",\n",
    "    \"u'w'\",\n",
    "    \"v'w'\",\n",
    "    \"k\"\n",
    "]\n",
    "\n",
    "vel_fluc_500_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deea3b4f-8a46-4670-8dae-2fb1b759ff94",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = \"../DNS (Lee and Moser 2015)/Couette Flow (Re -500)/LM_Couette_R0500_100PI_RSTE_uu_prof.dat\"    # file path is huge\n",
    "uu_500_df = pd.read_csv(\n",
    "    file,\n",
    "    sep='\\\\s+',\n",
    "    comment='%',\n",
    "    header=None\n",
    ")\n",
    "uu_500_df.columns = [\n",
    "    'y/delta',\n",
    "    'y^+',\n",
    "    \"Production\",\n",
    "    \"Turbulent_Transport\",\n",
    "    \"Viscous_Transport\",\n",
    "    \"Pressure_Strain\",\n",
    "    \"Pressure_Transport\",\n",
    "    \"Viscous_Dissipation\",\n",
    "    \"Balance\"\n",
    "]\n",
    "\n",
    "uu_500_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e7520e7-b93c-4caa-bc87-a53942b49bfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = \"../DNS (Lee and Moser 2015)/Couette Flow (Re -500)/LM_Couette_R0500_100PI_RSTE_vv_prof.dat\"    # file path is huge\n",
    "vv_500_df = pd.read_csv(\n",
    "    file,\n",
    "    sep='\\\\s+',\n",
    "    comment='%',\n",
    "    header=None\n",
    ")\n",
    "vv_500_df.columns = [\n",
    "    'y/delta',\n",
    "    'y^+',\n",
    "    \"Production\",\n",
    "    \"Turbulent_Transport\",\n",
    "    \"Viscous_Transport\",\n",
    "    \"Pressure_Strain\",\n",
    "    \"Pressure_Transport\",\n",
    "    \"Viscous_Dissipation\",\n",
    "    \"Balance\"\n",
    "]\n",
    "\n",
    "vv_500_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b304d725-2e75-4798-8bd3-55f93cc01fb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = \"../DNS (Lee and Moser 2015)/Couette Flow (Re -500)/LM_Couette_R0500_100PI_RSTE_ww_prof.dat\"    # file path is huge\n",
    "ww_500_df = pd.read_csv(\n",
    "    file,\n",
    "    sep='\\\\s+',\n",
    "    comment='%',\n",
    "    header=None\n",
    ")\n",
    "ww_500_df.columns = [\n",
    "    'y/delta',\n",
    "    'y^+',\n",
    "    \"Production\",\n",
    "    \"Turbulent_Transport\",\n",
    "    \"Viscous_Transport\",\n",
    "    \"Pressure_Strain\",\n",
    "    \"Pressure_Transport\",\n",
    "    \"Viscous_Dissipation\",\n",
    "    \"Balance\"\n",
    "]\n",
    "\n",
    "ww_500_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2fa199c-4c8e-41ca-aca6-390760223ea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = \"../DNS (Lee and Moser 2015)/Couette Flow (Re -500)/LM_Couette_R0500_100PI_RSTE_uv_prof.dat\"    # file path is huge\n",
    "uv_500_df = pd.read_csv(\n",
    "    file,\n",
    "    sep='\\\\s+',\n",
    "    comment='%',\n",
    "    header=None\n",
    ")\n",
    "uv_500_df.columns = [\n",
    "    'y/delta',\n",
    "    'y^+',\n",
    "    \"Production\",\n",
    "    \"Turbulent_Transport\",\n",
    "    \"Viscous_Transport\",\n",
    "    \"Pressure_Strain\",\n",
    "    \"Pressure_Transport\",\n",
    "    \"Viscous_Dissipation\",\n",
    "    \"Balance\"\n",
    "]\n",
    "\n",
    "uv_500_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "p6q7r8s9-t0u1-2v3w-4x5y-6z7a8b9c0d1e",
   "metadata": {},
   "source": [
    "## 3.3 Extract and Compute Required Features\n",
    "\n",
    "### Based on the column structure identified above, we now extract or compute:\n",
    "\n",
    "#### - **b₁₂**: Reynolds stress anisotropy = (uv)/(2k) where k = 0.5*(uu + vv + ww)\n",
    "#### - **dU/dy**: Mean velocity gradient (may be directly available or computed from U)\n",
    "#### - **ε**: Dissipation rate (should be in DNS output)\n",
    "#### - **k**: Turbulence kinetic energy\n",
    "#### - **φ₁₂**: Pressure-strain correlation (target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "q7r8s9t0-u1v2-3w4x-5y6z-7a8b9c0d1e2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "uu_500_df = uu_500_df.reset_index(drop=True)     # Ensure consistent ordering\n",
    "vv_500_df = vv_500_df.reset_index(drop=True)\n",
    "ww_500_df = ww_500_df.reset_index(drop=True)\n",
    "\n",
    "epsilon_500 = (uu_500_df['Viscous_Dissipation'] + vv_500_df['Viscous_Dissipation'] + ww_500_df['Viscous_Dissipation'])  \n",
    "epsilon_500_df = pd.DataFrame({        # Converting the series to DataFrame for merging it into the final Dataset\n",
    "    'y^+': uu_500_df['y^+'] ,\n",
    "    'epsilon': epsilon_500\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81895b5f-1a72-4ca5-ac6b-159ea1f1c52d",
   "metadata": {},
   "outputs": [],
   "source": [
    "vel_fluc_500_df = vel_fluc_500_df.sort_values('y^+').reset_index(drop=True)\n",
    "b_11_500 = vel_fluc_500_df[\"u'u'\"] / (2.0 * vel_fluc_500_df['k'] ) - 1.0 / 3.0  # All these four are Pandas Series\n",
    "b_22_500 = vel_fluc_500_df[\"v'v'\"] / (2.0 * vel_fluc_500_df['k']) - 1.0 / 3.0\n",
    "b_33_500 = vel_fluc_500_df[\"w'w'\"] / (2.0 * vel_fluc_500_df['k']) - 1.0 / 3.0\n",
    "b_12_500 = vel_fluc_500_df[\"u'v'\"] / (2.0 * vel_fluc_500_df['k'])\n",
    "\n",
    "b_500_df = pd.DataFrame({                      # Converting the series to DataFrame for merging it into the final Dataset     \n",
    "    'y^+': uu_500_df['y^+'],\n",
    "    'b_11': b_11_500,\n",
    "    'b_22': b_22_500,\n",
    "    'b_33': b_33_500,\n",
    "    'b_12': b_12_500\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a79b2d69-b153-48ec-b2cb-65bd321159bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "phi_500_df = pd.DataFrame({                      # Converting the series to DataFrame for merging it into the final Dataset     \n",
    "    'y^+': uu_500_df['y^+'],\n",
    "    'phi_11': uu_500_df['Pressure_Strain'],\n",
    "    'phi_22': vv_500_df['Pressure_Strain'],\n",
    "    'phi_33': ww_500_df['Pressure_Strain'],\n",
    "    'phi_12': uv_500_df['Pressure_Strain']\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e5230e9-49b1-4da7-a4fe-1d126510437a",
   "metadata": {},
   "outputs": [],
   "source": [
    "couette_processed = (\n",
    "    mean_prof_500_df[['y/delta','y^+', 'U']]\n",
    "    .merge(mean_prof_500_df[['y^+', 'dU/dy']], on='y^+')\n",
    "    .merge(vel_fluc_500_df[['y^+', 'k']], on='y^+')\n",
    "    .merge(vel_fluc_500_df[['y^+',\"u'u'\",\"v'v'\",\"w'w'\",\"u'v'\"]], on='y^+')\n",
    "    .merge(epsilon_500_df[['y^+', 'epsilon']], on='y^+')\n",
    "    .merge(b_500_df[['y^+', 'b_11', 'b_22', 'b_33', 'b_12']], on='y^+')\n",
    "    .merge(phi_500_df[['y^+','phi_11', 'phi_22', 'phi_33', 'phi_12']], on='y^+')\n",
    ")\n",
    "\n",
    "couette_processed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "r8s9t0u1-v2w3-4x5y-6z7a-8b9c0d1e2f3g",
   "metadata": {},
   "source": [
    "## 3.4 Data Quality Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "s9t0u1v2-w3x4-5y6z-7a8b-9c0d1e2f3g4h",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Data Quality Checks:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "print(\"\\n1. Missing Values:\")\n",
    "print(couette_processed.isnull().sum())\n",
    "\n",
    "print(\"\\n2. Infinite Values:\")\n",
    "print(np.isinf(couette_processed).sum())\n",
    "\n",
    "print(\"\\n3. Statistical Summary:\")\n",
    "print(couette_processed.describe())\n",
    "\n",
    "couette_clean = couette_processed.replace([np.inf, -np.inf], np.nan).dropna()         # Remove any NaN or Inf values\n",
    "\n",
    "if len(couette_clean) < len(couette_processed):\n",
    "    print(f\"\\nRemoved {len(couette_processed) - len(couette_clean)} rows with NaN/Inf values\")\n",
    "\n",
    "print(f\"\\nFinal clean data shape: {couette_clean.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "t0u1v2w3-x4y5-6z7a-8b9c-0d1e2f3g4h5i",
   "metadata": {},
   "source": [
    "## 3.5 Save as CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "u1v2w3x4-y5z6-7a8b-9c0d-1e2f3g4h5i6j",
   "metadata": {},
   "outputs": [],
   "source": [
    "couette_clean.to_csv(\"../Datasets/Couette-500.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "v2w3x4y5-z6a7-8b9c-0d1e-2f3g4h5i6j7k",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "w3x4y5z6-a7b8-9c0d-1e2f-3g4h5i6j7k8l",
   "metadata": {},
   "source": [
    "# 4. Load Trained Models and Scalers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "x4y5z6a7-b8c9-0d1e-2f3g-4h5i6j7k8l9m",
   "metadata": {},
   "source": [
    "## 4.1 Define MLP Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "y5z6a7b8-c9d0-1e2f-3g4h-5i6j7k8l9m0n",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_dim=4, hidden_dim=10, n_hidden_layers=5):\n",
    "        super().__init__()\n",
    "        \n",
    "        layers = []\n",
    "        layers.append(nn.Linear(input_dim, hidden_dim))\n",
    "        layers.append(nn.ReLU())\n",
    "        \n",
    "        for _ in range(n_hidden_layers - 1):\n",
    "            layers.append(nn.Linear(hidden_dim, hidden_dim))\n",
    "            layers.append(nn.ReLU())\n",
    "        \n",
    "        layers.append(nn.Linear(hidden_dim, 1))\n",
    "        \n",
    "        self.network = nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.network(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "z6a7b8c9-d0e1-2f3g-4h5i-6j7k8l9m0n1o",
   "metadata": {},
   "source": [
    "## 4.2 Load Models from All Cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7b8c9d0-e1f2-3g4h-5i6j-7k8l9m0n1o2p",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_models = {}\n",
    "gbdt_models = {}\n",
    "optimized_gbdt_models = {}\n",
    "mlp_models = {}\n",
    "scalers = {}\n",
    "\n",
    "for case_id in [1, 2, 3, 4]:\n",
    "    rf_models[case_id] = joblib.load(f\"../Models/Random Forest Models/rf_case{case_id}.joblib\")\n",
    "    gbdt_models[case_id] = joblib.load(f\"../Models/GBDT Models/gbdt_case{case_id}.joblib\")\n",
    "    optimized_gbdt_models[case_id] = joblib.load(f\"../Models/Optimized GBDT Models/optimized_gbdt_case{case_id}.joblib\")\n",
    "    scalers[case_id] = joblib.load(f\"../Models/Scalers/scaler_case{case_id}.joblib\")\n",
    "    \n",
    "    mlp = MLP(input_dim=4, hidden_dim=10, n_hidden_layers=5)\n",
    "    mlp.load_state_dict(torch.load(f\"../Models/MLP Models/mlp_case{case_id}.pth\"))\n",
    "    mlp.eval()\n",
    "    mlp_models[case_id] = mlp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8c9d0e1-f2g3-4h5i-6j7k-8l9m0n1o2p3q",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9d0e1f2-g3h4-5i6j-7k8l-9m0n1o2p3q4r",
   "metadata": {},
   "source": [
    "# 5. Prepare Couette Data for Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0e1f2g3-h4i5-6j7k-8l9m-0n1o2p3q4r5s",
   "metadata": {},
   "source": [
    "## 5.1 Extract Features and Target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1f2g3h4-i5j6-7k8l-9m0n-1o2p3q4r5s6t",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['b_12', 'dU/dy', 'epsilon', 'k']\n",
    "target = 'phi_12'\n",
    "\n",
    "X_couette = couette_clean[features].values\n",
    "y_couette_true = couette_clean[target].values\n",
    "y_plus = couette_clean['y^+'].values\n",
    "\n",
    "print(f\"Features shape: {X_couette.shape}\")\n",
    "print(f\"Target shape: {y_couette_true.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2g3h4i5-j6k7-8l9m-0n1o-2p3q4r5s6t7u",
   "metadata": {},
   "source": [
    "## 5.2 Scale Features Using Training Scalers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "g3h4i5j6-k7l8-9m0n-1o2p-3q4r5s6t7u8v",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_couette_scaled = {}\n",
    "\n",
    "for case_id in [1, 2, 3, 4]:\n",
    "    X_couette_scaled[case_id] = scalers[case_id].transform(X_couette)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "h4i5j6k7-l8m9-0n1o-2p3q-4r5s6t7u8v9w",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "i5j6k7l8-m9n0-1o2p-3q4r-5s6t7u8v9w0x",
   "metadata": {},
   "source": [
    "# 6. Generate Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "j6k7l8m9-n0o1-2p3q-4r5s-6t7u8v9w0x1y",
   "metadata": {},
   "outputs": [],
   "source": [
    "couette_predictions = {}\n",
    "\n",
    "for case_id in [1, 2, 3, 4]:\n",
    "    X_scaled = X_couette_scaled[case_id]\n",
    "    \n",
    "    rf_pred = rf_models[case_id].predict(X_scaled)\n",
    "    gbdt_pred = gbdt_models[case_id].predict(X_scaled)\n",
    "    opt_gbdt_pred = optimized_gbdt_models[case_id].predict(X_scaled)\n",
    "    \n",
    "    X_tensor = torch.FloatTensor(X_scaled)\n",
    "    with torch.no_grad():\n",
    "        mlp_pred = mlp_models[case_id](X_tensor).squeeze().numpy()\n",
    "    \n",
    "    couette_predictions[case_id] = {\n",
    "        'RF': rf_pred,\n",
    "        'GBDT': gbdt_pred,\n",
    "        'Optimized_GBDT': opt_gbdt_pred,\n",
    "        'MLP': mlp_pred\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "k7l8m9n0-o1p2-3q4r-5s6t-7u8v9w0x1y2z",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "l8m9n0o1-p2q3-4r5s-6t7u-8v9w0x1y2z3a",
   "metadata": {},
   "source": [
    "# 7. Performance Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "m9n0o1p2-q3r4-5s6t-7u8v-9w0x1y2z3a4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "\n",
    "for case_id in [1, 2, 3, 4]:\n",
    "    for model_name in ['RF', 'GBDT', 'Optimized_GBDT', 'MLP']:\n",
    "        y_pred = couette_predictions[case_id][model_name]\n",
    "        r2 = r2_score(y_couette_true, y_pred)\n",
    "        mse = mean_squared_error(y_couette_true, y_pred)\n",
    "        \n",
    "        results.append({\n",
    "            'Case': case_id,\n",
    "            'Model': model_name,\n",
    "            'R²': r2,\n",
    "            'MSE': mse\n",
    "        })\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"COUETTE FLOW OUT-OF-DISTRIBUTION TEST RESULTS\")\n",
    "print(\"=\"*60)\n",
    "pivot = results_df.pivot(index='Model', columns='Case', values='R²')\n",
    "print(pivot.to_string())\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"AVERAGE PERFORMANCE\")\n",
    "print(\"=\"*60)\n",
    "avg_perf = results_df.groupby('Model')['R²'].agg(['mean', 'std'])\n",
    "print(avg_perf.to_string())\n",
    "\n",
    "results_df.to_csv(\"../Tables/Couette_Flow_Results.csv\", index=False)\n",
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "n0o1p2q3-r4s5-6t7u-8v9w-0x1y2z3a4b5c",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "o1p2q3r4-s5t6-7u8v-9w0x-1y2z3a4b5c6d",
   "metadata": {},
   "source": [
    "# 8. Visualization: Figure 10 Recreation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "p2q3r4s5-t6u7-8v9w-0x1y-2z3a4b5c6d7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "case_id = 4       # Figure 10: Optimized GBDT on Couette flow\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "\n",
    "ax.plot(y_plus, y_couette_true, 'ko', markersize=5, markerfacecolor='none', label='DNS', zorder=3)\n",
    "ax.plot(y_plus, couette_predictions[case_id]['Optimized_GBDT'], 'r-', linewidth=2, label='Optimized GBDT')\n",
    "\n",
    "r2_val = r2_score(y_couette_true, couette_predictions[case_id]['Optimized_GBDT'])\n",
    "\n",
    "ax.set_xlabel('y⁺', fontsize=12)\n",
    "ax.set_ylabel('φ₁₂', fontsize=12)\n",
    "ax.set_title(f'Fig. 10: Prediction of the pressure strain correlation for turbulent Couette flow at Re_λ = 500\\nR² = {r2_val:.4f}', fontsize=11)\n",
    "ax.legend(loc='best')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../Plots/fig 10.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "q3r4s5t6-u7v8-9w0x-1y2z-3a4b5c6d7e8f",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "r4s5t6u7-v8w9-0x1y-2z3a-4b5c6d7e8f9g",
   "metadata": {},
   "source": [
    "# 9. Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "s5t6u7v8-w9x0-1y2z-3a4b-5c6d7e8f9g0h",
   "metadata": {},
   "source": [
    "## This phase successfully demonstrated out-of-distribution generalization of ML models trained on channel flow to turbulent Couette flow. The models' ability to predict a fundamentally different flow validates that they learned true turbulence physics rather than flow-specific patterns. This completes the comprehensive validation of the ML-based Reynolds stress transport modeling approach presented by J.P Panda & H.V Warrior (2021)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a2a8db9-bf86-4c1a-90ef-a4b6db7f61eb",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "631b5f27-5d88-4de5-bd74-b55c7f35fc51",
   "metadata": {},
   "source": [
    "# End of Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25462cce-4530-4789-adbc-10fe9f5a92d6",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
